{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = input('Input utilize gpu-id (cpu:-1): ')\n",
    "import sys\n",
    "sys.path.append('utils/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_integer('seq_length', 10,\n",
    "                            \"\"\"seq_length\"\"\")\n",
    "tf.app.flags.DEFINE_integer('seq_start', 4,\n",
    "                            \"\"\" start of seq generation\"\"\")\n",
    "tf.app.flags.DEFINE_integer('batch_size', 1,\n",
    "                            \"\"\"batch size for training\"\"\")\n",
    "tf.app.flags.DEFINE_integer('height', 128,\n",
    "                            \"\"\"height of input data\"\"\")\n",
    "tf.app.flags.DEFINE_integer('width', 128,\n",
    "                            \"\"\"width of input data\"\"\")\n",
    "tf.app.flags.DEFINE_string('warping', 'bilinear',\n",
    "                            \"\"\"method of warping\"\"\")\n",
    "tf.app.flags.DEFINE_integer('gating_num', 3,\n",
    "                            \"\"\"# of experts\"\"\")\n",
    "tf.app.flags.DEFINE_string('activation', 'softmax',\n",
    "                            \"\"\"gating activation\"\"\")\n",
    "tf.app.flags.DEFINE_boolean('restore_all', True, \n",
    "                            \"\"\"whether restore all or not \"\"\")\n",
    "tf.app.flags.DEFINE_boolean('restore_gating', True, \n",
    "                            \"\"\"whether restore gating or not \"\"\")\n",
    "tf.app.flags.DEFINE_integer('h_conv_ksize', 3,\n",
    "                            \"\"\"kernel size of H\"\"\")\n",
    "tf.app.flags.DEFINE_boolean('trainable_last', False, \n",
    "                            \"\"\"train backtopixel conv(H) of MIM or not\"\"\")\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import layer_def as ld\n",
    "from w_initializer import w_initializer\n",
    "from Expert import MIM\n",
    "from Gating import network_gate\n",
    "network_gate = tf.make_template('gate', network_gate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model1, model2, model3, model4, model5, model6, test_data):\n",
    "    \"\"\"\n",
    "    model1: model path for translation\n",
    "    model2: model path for rotation\n",
    "    model3: model path for growth/decay\n",
    "    model4: model path for H (decoder)\n",
    "    model5: model path for gating\n",
    "    model6: model path for the whole STMoE model\n",
    "    test_data: np.array for test\n",
    "    \"\"\"\n",
    "    x= tf.placeholder(tf.float32, [FLAGS.batch_size, FLAGS.height, FLAGS.width, FLAGS.seq_length])\n",
    "    x_g = []\n",
    "    weights, weights_last = [], []\n",
    "    h_s, h_r, h_g, h_g_all= [], [], [], []\n",
    "\n",
    "    hidden_state, hidden_state_diff, cell_state, cell_state_diff, st_memory = [], [], [], [], []\n",
    "    hidden_state_r, hidden_state_diff_r, cell_state_r, cell_state_diff_r, st_memory_r = [], [], [], [], []\n",
    "    hidden_state_s, hidden_state_diff_s, cell_state_s, cell_state_diff_s, st_memory_s = [], [], [], [], []  \n",
    "    \n",
    "    for i in range(FLAGS.seq_start - 1):\n",
    "            \n",
    "            with tf.variable_scope('expert2'):\n",
    "                inputs = x[:, :, :, i]\n",
    "                inputs = inputs[:, :, :, tf.newaxis]\n",
    "                x_generate_r, hidden_state_r, hidden_state_diff_r, cell_state_r, cell_state_diff_r, st_memory_r =                     MIM(inputs, i, hidden_state_r, hidden_state_diff_r, cell_state_r, cell_state_diff_r, st_memory_r, 3, [8, 8, 8], (3, 3), last_filter_size = FLAGS.h_conv_ksize, stride=1, tln = True)\n",
    "\n",
    "            with tf.variable_scope('expert1'):\n",
    "                inputs = x[:, :, :, i]\n",
    "                inputs = inputs[:, :, :, tf.newaxis]\n",
    "                x_generate_s, hidden_state_s, hidden_state_diff_s, cell_state_s, cell_state_diff_s, st_memory_s =                     MIM(inputs, i, hidden_state_s, hidden_state_diff_s, cell_state_s, cell_state_diff_s, st_memory_s, 3, [8, 8, 8], (3, 3), last_filter_size = FLAGS.h_conv_ksize, stride=1, tln = True)\n",
    "            \n",
    "            with tf.variable_scope('expert3'):\n",
    "                inputs = x[:, :, :, i]\n",
    "                inputs = inputs[:, :, :, tf.newaxis]\n",
    "                x_generate_g, hidden_state, hidden_state_diff, cell_state, cell_state_diff, st_memory =                     MIM(inputs, i, hidden_state, hidden_state_diff, cell_state, cell_state_diff, st_memory, 3, [8, 8, 8], (3, 3), last_filter_size = FLAGS.h_conv_ksize, stride=1, tln = True)\n",
    "                \n",
    "    for i in range(FLAGS.seq_length - FLAGS.seq_start):\n",
    "        print('frame_{}'.format(i))\n",
    "        if i == 0:\n",
    "            inputs = x[:, :, :,  FLAGS.seq_start - 1]\n",
    "            inputs = inputs[:, :, :, tf.newaxis]\n",
    "            last_x = x[:, :, :, FLAGS.seq_start - 1]\n",
    "            last_x = last_x[:, :, :, tf.newaxis]\n",
    "            x_input = x[:, :, :, i:FLAGS.seq_start]\n",
    "        else:\n",
    "            inputs = x_g[-1]\n",
    "            last_x = x_g[-1]\n",
    "            x_gen = tf.stack(x_g)\n",
    "            print(x_gen.shape)\n",
    "            x_gen = tf.transpose(x_gen[:, :, :, :, 0], [1, 2, 3, 0])\n",
    "\n",
    "            if i < FLAGS.seq_start:\n",
    "                x_input = tf.concat([x[:, :, :, i:FLAGS.seq_start], x_gen[:,:, :, :i]], axis = 3)\n",
    "            else:\n",
    "                x_input = x_gen[:, :, :, i - FLAGS.seq_start:i]\n",
    "        with tf.variable_scope('expert3'):\n",
    "            x_generate_g, hidden_state, hidden_state_difff, cell_state, cell_state_diff, st_memory =                 MIM(inputs, i + FLAGS.seq_start - 1, hidden_state, hidden_state_diff, cell_state, cell_state_diff, st_memory, 3, [8, 8, 8], (3, 3), last_filter_size = FLAGS.h_conv_ksize, stride=1, tln = True)\n",
    "            h_g.append(hidden_state[-1])\n",
    "            \n",
    "        with tf.variable_scope('expert1'):  \n",
    "            x_generate_s, hidden_state_s, hidden_state_difff_s, cell_state_s, cell_state_diff_s, st_memory_s =                 MIM(inputs, i + FLAGS.seq_start - 1, hidden_state_s, hidden_state_diff_s, cell_state_s, cell_state_diff_s, st_memory_s, 3, [8, 8, 8], (3, 3), last_filter_size = FLAGS.h_conv_ksize, stride=1, tln = True)\n",
    "            h_s.append(hidden_state_s[-1])\n",
    "            \n",
    "        with tf.variable_scope('expert2'):   \n",
    "            x_generate_r, hidden_state_r, hidden_state_difff_r, cell_state_r, cell_state_diff_r, st_memory_r =                 MIM(inputs, i + FLAGS.seq_start - 1, hidden_state_r, hidden_state_diff_r, cell_state_r, cell_state_diff_r, st_memory_r, 3, [8, 8, 8], (3, 3), last_filter_size = FLAGS.h_conv_ksize, stride=1, tln = True)\n",
    "            h_r.append(hidden_state_r[-1]) \n",
    "\n",
    "        \n",
    "        with tf.variable_scope('gating_network', reuse = tf.AUTO_REUSE):\n",
    "            if i == 0:\n",
    "                x_input = x[:, :, :, i:i+FLAGS.seq_start]\n",
    "            elif i < FLAGS.seq_start:\n",
    "                x_input = tf.concat([x[:, :, :, i:FLAGS.seq_start], x_gen[:,:, :, :i]], axis = 3)\n",
    "            else:\n",
    "                x_input = x_gen[:, :, :, i - FLAGS.seq_start:i]\n",
    "            \n",
    "            weight = network_gate(x_input, FLAGS.gating_num, FLAGS.activation)\n",
    "            h_generate = weight[:, :, :, 0][:, :, :, tf.newaxis] * h_s[-1] + weight[:, :, :, 1][:, :, :, tf.newaxis] * h_r[-1] + weight[:, :, :, 2][:, :, :, tf.newaxis] * h_g[-1] \n",
    "            \n",
    "            x_generate = tf.layers.conv2d(h_generate,\n",
    "                                     filters= 1,\n",
    "                                     kernel_size=FLAGS.h_conv_ksize,\n",
    "                                     strides=1,\n",
    "                                     padding='same',\n",
    "                                     kernel_initializer=w_initializer(8, 1),\n",
    "                                     trainable = False,\n",
    "                                     name=\"back_to_pixel\")\n",
    "            h_g_all.append(h_generate)\n",
    "            x_g.append(x_generate)\n",
    "            weights.append(weight)\n",
    "            x_gen = tf.stack(x_g)\n",
    "            x_gen = tf.transpose(x_gen[:, :, :, :, 0], [1, 2, 3, 0])\n",
    "            \n",
    "            w_generate_s = tf.layers.conv2d(tf.tile(weight[:, :, :, 0:1], [1, 1, 1, 8]),\n",
    "                                 filters= 1,\n",
    "                                 kernel_size=FLAGS.h_conv_ksize,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 kernel_initializer=w_initializer(8, 1),\n",
    "                                 trainable = False,\n",
    "                                 name=\"back_to_pixel\")\n",
    "            w_generate_r = tf.layers.conv2d(tf.tile(weight[:, :, :, 1:2], [1, 1, 1, 8]),\n",
    "                                 filters= 1,\n",
    "                                 kernel_size=FLAGS.h_conv_ksize,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 kernel_initializer=w_initializer(8, 1),\n",
    "                                 trainable = False,\n",
    "                                 name=\"back_to_pixel\")\n",
    "            w_generate_g = tf.layers.conv2d(tf.tile(weight[:, :, :, 2:3], [1, 1, 1, 8]),\n",
    "                                 filters= 1,\n",
    "                                 kernel_size=FLAGS.h_conv_ksize,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 kernel_initializer=w_initializer(8, 1),\n",
    "                                 trainable = False,\n",
    "                                 name=\"back_to_pixel\")\n",
    "                   \n",
    "            w_generate = tf.concat([w_generate_s, w_generate_r, w_generate_g], axis = -1)\n",
    "            \n",
    "            weights_last.append(w_generate)\n",
    "\n",
    "\n",
    "    x_g = tf.stack(x_g)\n",
    "    x_g = tf.transpose(x_g[:, :, :, :, 0], [1, 2, 3, 0])\n",
    "    print(x_g.shape)\n",
    "    \n",
    "    weights = tf.stack(weights)\n",
    "\n",
    "\n",
    "    MSE = tf.losses.mean_squared_error(x[:, :, :, FLAGS.seq_start:], x_g[:, :, :, :])\n",
    "\n",
    "    # SSIM\n",
    "    SSIM = tf.image.ssim(x[:, :, :, FLAGS.seq_start:], x_g[:, :, :, :], 1)\n",
    "\n",
    "    # List of all varables\n",
    "    variables = tf.global_variables()\n",
    "    # strat rinning operations on Graph\n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    print('init netwrok from scratch....')\n",
    "    sess.run(init)\n",
    "    \n",
    "\n",
    "    # restore experts or build train saver\n",
    "    expert1_varlist = {v.op.name[8:]: v\n",
    "          for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"expert1/\")}\n",
    "    expert1_saver = tf.train.Saver(var_list=expert1_varlist)\n",
    "\n",
    "    expert2_varlist = {v.op.name[8:]: v\n",
    "          for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"expert2/\")}\n",
    "    expert2_saver = tf.train.Saver(var_list=expert2_varlist)\n",
    "\n",
    "    expert3_varlist = {v.op.name[8:]: v\n",
    "          for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"expert3/\")}\n",
    "    expert3_saver = tf.train.Saver(var_list=expert3_varlist)\n",
    "\n",
    "    # restore the saver\n",
    "    expert1_saver.restore(sess, model1)\n",
    "\n",
    "    expert2_saver.restore(sess, model2)\n",
    "    \n",
    "    expert3_saver.restore(sess, model3)\n",
    "    \n",
    "    # build and restore gating network \n",
    "    gating_varlist = {v.name.lstrip(\"gating_network/\"): v\n",
    "    for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"gating_network/\")}\n",
    "    gating_saver = tf.train.Saver(var_list=gating_varlist, max_to_keep=100)\n",
    "    \n",
    "    if FLAGS.restore_gating:\n",
    "        gating_saver.restore(sess, model5)\n",
    "        \n",
    "    # restore back to pixel (H)\n",
    "    back2pixcel_varlist = {'pred_rnn/'+ v.op.name[15:]: v\n",
    "      for v in tf.get_collection(tf.GraphKeys.VARIABLES, scope=\"gating_network/back_to_pixel\")}\n",
    "    print(back2pixcel_varlist)\n",
    "    back2pixel_saver = tf.train.Saver(var_list=back2pixcel_varlist)\n",
    "    back2pixel_saver.restore(sess, model4)\n",
    "    \n",
    "    # build all saver\n",
    "    all_varlist = {v.name: v\n",
    "    for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)}\n",
    "    all_saver =  tf.train.Saver(all_varlist, max_to_keep=100)\n",
    "\n",
    "    if FLAGS.restore_all:\n",
    "        all_saver.restore(sess, model6)\n",
    "            \n",
    "    Loss, SSIM_test = [], []\n",
    "    Pred, w_G_l = [], []\n",
    "    \n",
    "    # test\n",
    "    for idx in range(0, test_data.shape[0], FLAGS.batch_size):\n",
    "        test_da = test_data[idx:idx+FLAGS.batch_size, :, :, :].transpose(0, 2, 3, 1)\n",
    "        MSE_loss, x_gene, w_gene_l, SSIM_loss =  sess.run([MSE, x_g, weights_last, SSIM ], feed_dict = {x:test_da})\n",
    "        Loss.append(MSE_loss)\n",
    "        Pred.append(x_gene)\n",
    "        w_G_l.append(w_gene_l)\n",
    "        SSIM_test.append(SSIM_loss)\n",
    "        \n",
    "    return Loss, Pred, w_G_l, SSIM_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    model1 = './models/STMoE-2_trans'\n",
    "    model2 = './models/STMoE-2_rot'\n",
    "    model3 = './models/STMoE-2_grow' \n",
    "    model4 = './models/STMoE-2_H' \n",
    "    model5 = './models/STMoE-2_gating'\n",
    "    model6 = './models/STMoE-2_all'\n",
    "\n",
    "    test_data = np.load('./data/test_data.npy')\n",
    "    max_intensity = 255\n",
    "        \n",
    "    test_data = test_data / max_intensity\n",
    "    \n",
    "    Loss, Pred, w_G_l, SSIM_test = test(model1, model2, model3, model4, model5, model6, test_data)\n",
    "    \n",
    "    return Loss, Pred, w_G_l, SSIM_test, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    Loss, Pred, w_G_l, SSIM_test, test_data = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loss = np.array(Loss)*255**2\n",
    "Loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSIM = np.array(SSIM_test)\n",
    "SSIM.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pred = np.array(Pred)*255\n",
    "Pred = Pred[:, 0, :, :, :]\n",
    "test_data = test_data*255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 79\n",
    "vmx = 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pred vs gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(FLAGS.seq_length):   \n",
    "    if i >= FLAGS.seq_start:\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(Pred[num, :, :, i-FLAGS.seq_start], vmin = 0, vmax = vmx, cmap = 'jet') \n",
    "        plt.title('est_{}'.format(i))\n",
    "        plt.colorbar()\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(test_data[num, i, :, :], vmin = 0, vmax = vmx, cmap = 'jet')   \n",
    "    plt.title('gt_{}'.format(i))\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "for i in range(FLAGS.seq_length):   \n",
    "    if i >= FLAGS.seq_start:\n",
    "        plt.subplot(1, 10, i+1)\n",
    "        plt.imshow(Pred[num, :, :, i-FLAGS.seq_start], vmin = 0, vmax = vmx, cmap = 'jet') \n",
    "        plt.title('est_{}'.format(i))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pred and contribution of each experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_g_l = np.array(w_G_l)\n",
    "w_g_l = w_g_l[:, :, 0]\n",
    "w_g_l.shape\n",
    "w_g_l_ = abs(w_g_l) / np.sum(abs(w_g_l), axis = 4)[:, :, :, :, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = [79]\n",
    "k = ['translation', 'rotation', 'g/d']\n",
    "t = 0\n",
    "axes = []\n",
    "fig=plt.figure(figsize=(12, 9))\n",
    "for i, n in enumerate(nums):\n",
    "    for j in range(3):\n",
    "        axes.append(fig.add_subplot(4, 4, ((4*i+j+1))))\n",
    "        plt.imshow(Pred[n, :, :, t], cmap ='jet', vmin=0, vmax=vmx)\n",
    "        plt.imshow(abs(w_g_l_[n, t, :, :, j])/(w_g_l_.max()), alpha=0.6, vmax= 0.8)\n",
    "        plt.title('{}'.format(k[j]), fontsize=15)\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
